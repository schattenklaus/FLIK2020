{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder with MNIST and FashionMNIST\n",
    "\n",
    "We will use Mnist and the Zalando FashionMNIST again, to train a variational autoencoder with will also be able to generate new cloth.\n",
    "\n",
    "As a source for this notebook, see [https://blog.keras.io/building-autoencoders-in-keras.html]. A another accurate example can be found here: [https://towardsdatascience.com/teaching-a-variational-autoencoder-vae-to-draw-mnist-characters-978675c95776] .\n",
    "\n",
    "To begin, we need to load some python modules including common layers from keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 14, 14, 8)    80          encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 14, 14, 8)    32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 7, 7, 16)     1168        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 7, 7, 16)     64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 784)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 20)           15700       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20)           80          dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "latent_mu (Dense)               (None, 2)            42          batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "latent_sigma (Dense)            (None, 2)            42          batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 2)            0           latent_mu[0][0]                  \n",
      "                                                                 latent_sigma[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 17,208\n",
      "Trainable params: 17,120\n",
      "Non-trainable params: 88\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 784)               2352      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 16)        2320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 14, 14, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 8)         1160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 28, 28, 8)         32        \n",
      "_________________________________________________________________\n",
      "decoder_output (Conv2DTransp (None, 28, 28, 1)         73        \n",
      "=================================================================\n",
      "Total params: 9,137\n",
      "Trainable params: 7,521\n",
      "Non-trainable params: 1,616\n",
      "_________________________________________________________________\n",
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 2), (None, 2), (N 17208     \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 28, 28, 1)         9137      \n",
      "=================================================================\n",
      "Total params: 26,345\n",
      "Trainable params: 24,641\n",
      "Non-trainable params: 1,704\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 22s 467us/step - loss: 269.9646 - val_loss: 235.8809\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 8s 164us/step - loss: 170.9458 - val_loss: 164.7468\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 8s 164us/step - loss: 163.8659 - val_loss: 160.0358\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 8s 165us/step - loss: 161.2396 - val_loss: 158.7762\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 8s 165us/step - loss: 159.5996 - val_loss: 156.6329\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 8s 166us/step - loss: 158.5113 - val_loss: 155.8596\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 8s 165us/step - loss: 157.7245 - val_loss: 155.3088\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 8s 165us/step - loss: 156.9549 - val_loss: 154.4615\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 8s 164us/step - loss: 156.4565 - val_loss: 153.6605\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 8s 164us/step - loss: 155.7890 - val_loss: 153.3888\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 8s 165us/step - loss: 155.6655 - val_loss: 153.2768\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 8s 165us/step - loss: 155.3522 - val_loss: 153.4735\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 8s 165us/step - loss: 155.1219 - val_loss: 152.2407\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 8s 166us/step - loss: 155.0249 - val_loss: 151.9455\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 8s 165us/step - loss: 154.5187 - val_loss: 152.0325\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 8s 164us/step - loss: 154.5508 - val_loss: 151.8835\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 8s 166us/step - loss: 154.2202 - val_loss: 151.1793\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 8s 164us/step - loss: 154.0660 - val_loss: 152.4878\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 8s 165us/step - loss: 154.0466 - val_loss: 151.6651\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 8s 166us/step - loss: 153.7163 - val_loss: 152.2289\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 8s 163us/step - loss: 153.6065 - val_loss: 151.7395\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 8s 165us/step - loss: 153.6292 - val_loss: 150.8097\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 8s 165us/step - loss: 153.5580 - val_loss: 151.8824\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 8s 164us/step - loss: 153.1454 - val_loss: 151.5625\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 8s 162us/step - loss: 153.1494 - val_loss: 150.8039\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 8s 166us/step - loss: 152.9735 - val_loss: 150.8527\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 8s 164us/step - loss: 152.7776 - val_loss: 150.2889\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 8s 163us/step - loss: 152.6909 - val_loss: 150.3943\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 8s 165us/step - loss: 152.8831 - val_loss: 153.0567\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 8s 163us/step - loss: 152.7056 - val_loss: 150.9726\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 8s 165us/step - loss: 152.4123 - val_loss: 150.2749\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 8s 166us/step - loss: 152.4592 - val_loss: 150.8594\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 8s 164us/step - loss: 152.2763 - val_loss: 150.6048\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 8s 164us/step - loss: 152.2042 - val_loss: 150.3107\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 8s 165us/step - loss: 152.1104 - val_loss: 152.0765\n",
      "Epoch 36/100\n",
      "48000/48000 [==============================] - 8s 162us/step - loss: 152.1533 - val_loss: 149.9868\n",
      "Epoch 37/100\n",
      "48000/48000 [==============================] - 8s 163us/step - loss: 152.0968 - val_loss: 151.0838\n",
      "Epoch 38/100\n",
      "48000/48000 [==============================] - 8s 166us/step - loss: 151.8638 - val_loss: 150.6931\n",
      "Epoch 39/100\n",
      "48000/48000 [==============================] - 8s 165us/step - loss: 151.9716 - val_loss: 149.7589\n",
      "Epoch 40/100\n",
      "48000/48000 [==============================] - 8s 165us/step - loss: 151.8436 - val_loss: 149.9076\n",
      "Epoch 41/100\n",
      "48000/48000 [==============================] - 8s 166us/step - loss: 151.6587 - val_loss: 150.9682\n",
      "Epoch 42/100\n",
      "48000/48000 [==============================] - 8s 165us/step - loss: 151.9629 - val_loss: 149.6622\n",
      "Epoch 43/100\n",
      "48000/48000 [==============================] - 8s 164us/step - loss: 151.6168 - val_loss: 149.5047\n",
      "Epoch 44/100\n",
      "48000/48000 [==============================] - 8s 164us/step - loss: 151.6158 - val_loss: 149.5287\n",
      "Epoch 45/100\n",
      "48000/48000 [==============================] - 8s 165us/step - loss: 151.5308 - val_loss: 149.9964\n",
      "Epoch 46/100\n",
      "48000/48000 [==============================] - 8s 165us/step - loss: 151.6837 - val_loss: 149.2928\n",
      "Epoch 47/100\n",
      "48000/48000 [==============================] - 8s 165us/step - loss: 151.5162 - val_loss: 149.2612\n",
      "Epoch 48/100\n",
      "48000/48000 [==============================] - 8s 165us/step - loss: 151.5975 - val_loss: 150.0437\n",
      "Epoch 49/100\n",
      "48000/48000 [==============================] - 8s 164us/step - loss: 151.5456 - val_loss: 149.1776\n",
      "Epoch 50/100\n",
      "48000/48000 [==============================] - 8s 165us/step - loss: 151.3886 - val_loss: 150.2447\n",
      "Epoch 51/100\n",
      "48000/48000 [==============================] - 8s 165us/step - loss: 151.2605 - val_loss: 149.2258\n",
      "Epoch 52/100\n",
      "48000/48000 [==============================] - 8s 164us/step - loss: 150.9926 - val_loss: 149.6045\n",
      "Epoch 53/100\n",
      "48000/48000 [==============================] - 8s 164us/step - loss: 151.3539 - val_loss: 149.1854\n",
      "Epoch 54/100\n",
      "48000/48000 [==============================] - 8s 166us/step - loss: 151.2079 - val_loss: 149.5935\n",
      "Epoch 55/100\n",
      "36480/48000 [=====================>........] - ETA: 1s - loss: 150.6667"
     ]
    }
   ],
   "source": [
    "'''\n",
    "  Variational Autoencoder (VAE) with the Keras Functional API.\n",
    "'''\n",
    "\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST dataset\n",
    "(input_train, target_train), (input_test, target_test) = mnist.load_data()\n",
    "\n",
    "# Data & model configuration\n",
    "img_width, img_height = input_train.shape[1], input_train.shape[2]\n",
    "batch_size = 128\n",
    "no_epochs = 100\n",
    "validation_split = 0.2\n",
    "verbosity = 1\n",
    "latent_dim = 2\n",
    "num_channels = 1\n",
    "\n",
    "# Reshape data\n",
    "input_train = input_train.reshape(input_train.shape[0], img_height, img_width, num_channels)\n",
    "input_test = input_test.reshape(input_test.shape[0], img_height, img_width, num_channels)\n",
    "input_shape = (img_height, img_width, num_channels)\n",
    "\n",
    "# Parse numbers as floats\n",
    "input_train = input_train.astype('float32')\n",
    "input_test = input_test.astype('float32')\n",
    "\n",
    "# Normalize data\n",
    "input_train = input_train / 255\n",
    "input_test = input_test / 255\n",
    "\n",
    "# # =================\n",
    "# # Encoder\n",
    "# # =================\n",
    "\n",
    "# Definition\n",
    "i       = Input(shape=input_shape, name='encoder_input')\n",
    "cx      = Conv2D(filters=8, kernel_size=3, strides=2, padding='same', activation='relu')(i)\n",
    "cx      = BatchNormalization()(cx)\n",
    "cx      = Conv2D(filters=16, kernel_size=3, strides=2, padding='same', activation='relu')(cx)\n",
    "cx      = BatchNormalization()(cx)\n",
    "x       = Flatten()(cx)\n",
    "x       = Dense(20, activation='relu')(x)\n",
    "x       = BatchNormalization()(x)\n",
    "mu      = Dense(latent_dim, name='latent_mu')(x)\n",
    "sigma   = Dense(latent_dim, name='latent_sigma')(x)\n",
    "\n",
    "# Get Conv2D shape for Conv2DTranspose operation in decoder\n",
    "conv_shape = K.int_shape(cx)\n",
    "\n",
    "# Define sampling with reparameterization trick\n",
    "def sample_z(args):\n",
    "  mu, sigma = args\n",
    "  batch     = K.shape(mu)[0]\n",
    "  dim       = K.int_shape(mu)[1]\n",
    "  eps       = K.random_normal(shape=(batch, dim))\n",
    "  return mu + K.exp(sigma / 2) * eps\n",
    "\n",
    "# Use reparameterization trick to ....??\n",
    "z       = Lambda(sample_z, output_shape=(latent_dim, ), name='z')([mu, sigma])\n",
    "\n",
    "# Instantiate encoder\n",
    "encoder = Model(i, [mu, sigma, z], name='encoder')\n",
    "encoder.summary()\n",
    "\n",
    "# =================\n",
    "# Decoder\n",
    "# =================\n",
    "\n",
    "# Definition\n",
    "d_i   = Input(shape=(latent_dim, ), name='decoder_input')\n",
    "x     = Dense(conv_shape[1] * conv_shape[2] * conv_shape[3], activation='relu')(d_i)\n",
    "x     = BatchNormalization()(x)\n",
    "x     = Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(x)\n",
    "cx    = Conv2DTranspose(filters=16, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
    "cx    = BatchNormalization()(cx)\n",
    "cx    = Conv2DTranspose(filters=8, kernel_size=3, strides=2, padding='same',  activation='relu')(cx)\n",
    "cx    = BatchNormalization()(cx)\n",
    "o     = Conv2DTranspose(filters=num_channels, kernel_size=3, activation='sigmoid', padding='same', name='decoder_output')(cx)\n",
    "\n",
    "# Instantiate decoder\n",
    "decoder = Model(d_i, o, name='decoder')\n",
    "decoder.summary()\n",
    "\n",
    "# =================\n",
    "# VAE as a whole\n",
    "# =================\n",
    "\n",
    "# Instantiate VAE\n",
    "vae_outputs = decoder(encoder(i)[2])\n",
    "vae         = Model(i, vae_outputs, name='vae')\n",
    "vae.summary()\n",
    "\n",
    "# Define loss\n",
    "def kl_reconstruction_loss(true, pred):\n",
    "  # Reconstruction loss\n",
    "  reconstruction_loss = binary_crossentropy(K.flatten(true), K.flatten(pred)) * img_width * img_height\n",
    "  # KL divergence loss\n",
    "  kl_loss = 1 + sigma - K.square(mu) - K.exp(sigma)\n",
    "  kl_loss = K.sum(kl_loss, axis=-1)\n",
    "  kl_loss *= -0.5\n",
    "  # Total loss = 50% rec + 50% KL divergence loss\n",
    "  return K.mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "# Compile VAE\n",
    "vae.compile(optimizer='adam', loss=kl_reconstruction_loss)\n",
    "\n",
    "# Train autoencoder\n",
    "vae.fit(input_train, input_train, epochs = no_epochs, batch_size = batch_size, validation_split = validation_split)\n",
    "\n",
    "# =================\n",
    "# Results visualization\n",
    "# Credits for original visualization code: https://keras.io/examples/variational_autoencoder_deconv/\n",
    "# (François Chollet).\n",
    "# Adapted to accomodate this VAE.\n",
    "# =================\n",
    "def viz_latent_space(encoder, data):\n",
    "  input_data, target_data = data\n",
    "  mu, _, _ = encoder.predict(input_data)\n",
    "  plt.figure(figsize=(8, 10))\n",
    "  plt.scatter(mu[:, 0], mu[:, 1], c=target_data)\n",
    "  plt.xlabel('z - dim 1')\n",
    "  plt.ylabel('z - dim 2')\n",
    "  plt.colorbar()\n",
    "  plt.show()\n",
    "\n",
    "def viz_decoded(encoder, decoder, data):\n",
    "  num_samples = 15\n",
    "  figure = np.zeros((img_width * num_samples, img_height * num_samples, num_channels))\n",
    "  grid_x = np.linspace(-4, 4, num_samples)\n",
    "  grid_y = np.linspace(-4, 4, num_samples)[::-1]\n",
    "  for i, yi in enumerate(grid_y):\n",
    "      for j, xi in enumerate(grid_x):\n",
    "          z_sample = np.array([[xi, yi]])\n",
    "          x_decoded = decoder.predict(z_sample)\n",
    "          digit = x_decoded[0].reshape(img_width, img_height, num_channels)\n",
    "          figure[i * img_width: (i + 1) * img_width,\n",
    "                  j * img_height: (j + 1) * img_height] = digit\n",
    "  plt.figure(figsize=(10, 10))\n",
    "  start_range = img_width // 2\n",
    "  end_range = num_samples * img_width + start_range + 1\n",
    "  pixel_range = np.arange(start_range, end_range, img_width)\n",
    "  sample_range_x = np.round(grid_x, 1)\n",
    "  sample_range_y = np.round(grid_y, 1)\n",
    "  plt.xticks(pixel_range, sample_range_x)\n",
    "  plt.yticks(pixel_range, sample_range_y)\n",
    "  plt.xlabel('z - dim 1')\n",
    "  plt.ylabel('z - dim 2')\n",
    "  # matplotlib.pyplot.imshow() needs a 2D array, or a 3D array with the third dimension being of shape 3 or 4!\n",
    "  # So reshape if necessary\n",
    "  fig_shape = np.shape(figure)\n",
    "  if fig_shape[2] == 1:\n",
    "    figure = figure.reshape((fig_shape[0], fig_shape[1]))\n",
    "  # Show image\n",
    "  plt.imshow(figure)\n",
    "  plt.show()\n",
    "\n",
    "# Plot results\n",
    "data = (input_test, target_test)\n",
    "viz_latent_space(encoder, data)\n",
    "viz_decoded(encoder, decoder, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset\n",
    "from keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "\n",
    "# numpy and pyplot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# keras\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Flatten, Reshape, Conv2D, MaxPooling2D, UpSampling2D, Dropout, BatchNormalization\n",
    "from keras.layers import Multiply, Add, GaussianNoise, Lambda\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare the data by normalizing it.\n",
    "\n",
    "Sincle we are doing unsupervised learning here, we will not need the labels provided by the dataset for now. We keep them however, as they will help with visualizing the results later.\n",
    "\n",
    "There are 60k training and 10k test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In priciple, an autoencoder consists of two models: the encoder and the decoder. To represent this, we are using keras' functional API where we can easily define models from component models.\n",
    "\n",
    "We start by defining the encoder, whose output will no be the latent vector, but the mean and log of the standard deviation of the latent representations.\n",
    "\n",
    "The next submodel is on that sample from the distribution generated by the encoder.\n",
    "\n",
    "Then we define the decoder, which takes the sampled latent vector as input and produces full-size images again.\n",
    "\n",
    "Finally, we concatenate everything to get our variational autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeVAE(encodingDim=32):\n",
    "    # this is our input placeholder\n",
    "    inputImg = Input(shape=x_train.shape[1:])\n",
    "    x = Reshape((*inputImg.shape.as_list()[1:],1))(inputImg)\n",
    "    # encoder\n",
    "    x = Conv2D(16, kernel_size=(5,5), activation='relu', padding=\"same\")(x)\n",
    "    x = Conv2D(32, kernel_size=(3,3), activation='relu', padding=\"same\")(x)\n",
    "    x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(64, kernel_size=(3,3), activation='relu', padding=\"same\")(x)\n",
    "    x = Conv2D(32, kernel_size=(1,1), activation='relu', padding=\"same\")(x)\n",
    "    x = Conv2D(64, kernel_size=(3,3), activation='relu', padding=\"same\")(x)\n",
    "    x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128, kernel_size=(3,3), activation='relu', padding=\"same\")(x)\n",
    "    x = Conv2D(64, kernel_size=(1,1), activation='relu', padding=\"same\")(x)\n",
    "    x = Conv2D(128, kernel_size=(3,3), activation='relu', padding=\"same\")(x)\n",
    "    x = Conv2D(64, kernel_size=(1,1), activation='relu', padding=\"same\")(x)\n",
    "    x = Conv2D(64, kernel_size=(3,3), activation='relu', padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(32, kernel_size=(1,1), activation='relu', padding=\"same\")(x)\n",
    "    lastConvShape = x.shape.as_list()[1:]\n",
    "    \n",
    "    x = Flatten()(x)    \n",
    "    x = Dense(encodingDim*4, activation='relu')(x)\n",
    "    x = Dense(encodingDim*2, activation='relu')(x)\n",
    "    x = Dense(encodingDim, activation='relu')(x)\n",
    "    \n",
    "    mean = Dense(encodingDim)(x)\n",
    "    logstdev = Dense(encodingDim)(x)\n",
    "    \n",
    "    encoder = Model(inputImg, [mean, logstdev], name=\"encoder\")\n",
    "    encoder.summary()\n",
    "\n",
    "    def sampling(args):\n",
    "        mean, logstdev = args\n",
    "        eps = K.random_normal(shape=(K.shape(logstdev)[0], encodingDim))\n",
    "        return mean + K.exp(logstdev) * eps\n",
    "\n",
    "    meanS = Input(shape=(encodingDim,))\n",
    "    logstdevS = Input(shape=(encodingDim,))\n",
    "    x = Lambda(sampling)([meanS, logstdevS])\n",
    "    sample = Model([meanS, logstdevS], x, name=\"sample\")\n",
    "    sample.summary()\n",
    "    \n",
    "    # this is our latent space placeholder\n",
    "    inputLat = Input(shape=(encodingDim,))\n",
    "    #decoder\n",
    "    x = Dense(encodingDim*4, activation='relu')(inputLat)\n",
    "    x = Dense(np.prod(lastConvShape), activation='relu')(x)\n",
    "    \n",
    "    x = Reshape(lastConvShape)(x) # remove channel dimension\n",
    "    x = Conv2D(64, kernel_size=(3,3), activation='relu', padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(64, kernel_size=(1,1), activation='relu', padding=\"same\")(x)\n",
    "    x = Conv2D(128, kernel_size=(3,3), activation='relu', padding=\"same\")(x)\n",
    "    x = Conv2D(64, kernel_size=(1,1), activation='relu', padding=\"same\")(x)\n",
    "    x = Conv2D(128, kernel_size=(3,3), activation='relu', padding=\"same\")(x)\n",
    "    x = UpSampling2D(size=(2,2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(64, kernel_size=(3,3), activation='relu', padding=\"same\")(x)\n",
    "    x = Conv2D(32, kernel_size=(1,1), activation='relu', padding=\"same\")(x)\n",
    "    x = Conv2D(64, kernel_size=(3,3), activation='relu', padding=\"same\")(x)\n",
    "    x = UpSampling2D(size=(2,2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(32, kernel_size=(3,3), activation='relu', padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(1, kernel_size=(5,5), activation='sigmoid', padding=\"same\")(x)\n",
    "    x = Reshape(x.shape.as_list()[1:3])(x) # remove channel dimension\n",
    "\n",
    "    \n",
    "    decoder = Model(inputLat, x, name=\"decoder\")\n",
    "    decoder.summary()\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    autoencoder = Model(inputImg, decoder(sample(encoder(inputImg))), name=\"vae\")\n",
    "    autoencoder.summary()\n",
    "    \n",
    "    def loss(x, output):\n",
    "        recon_loss = K.sum(binary_crossentropy(x, output))\n",
    "        \"\"\"\n",
    "        This is quite dirty: using the layer handles from the definition of the encoder to calculate the loss.\n",
    "        It would be better to have these values as additional outputs of the network,\n",
    "        but keras does not allow passing multiple outputs into a single loss function.\n",
    "        \"\"\"\n",
    "        kl_loss = - 0.5 * K.mean(1. + 2.*logstdev - K.square(mean) - K.exp(2.*logstdev), axis=-1)\n",
    "        return recon_loss + kl_loss\n",
    "        #return kl_loss\n",
    "    \n",
    "    return encoder, decoder, autoencoder, sample, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 28, 28)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 28, 28, 1)    0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 28, 28, 16)   416         reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 28, 28, 32)   4640        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 14, 14, 32)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 14, 14, 32)   128         max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 14, 14, 64)   18496       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 14, 14, 32)   2080        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 14, 14, 64)   18496       conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 7, 7, 64)     0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 7, 7, 64)     256         max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 7, 7, 128)    73856       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 7, 7, 64)     8256        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 7, 7, 128)    73856       conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 7, 7, 64)     8256        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 7, 7, 64)     36928       conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 7, 7, 64)     256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 7, 7, 32)     2080        batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1568)         0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           100416      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           2080        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           528         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           272         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           272         dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 351,568\n",
      "Trainable params: 351,248\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Duplicate node name in graph: 'lambda_1/random_normal/shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/software/ml/JupyterHub/conda-env-20200827-2252/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1618\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1619\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1620\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Duplicate node name in graph: 'lambda_1/random_normal/shape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3fc49faebac1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmakeVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-198a871d77cc>\u001b[0m in \u001b[0;36mmakeVAE\u001b[0;34m(encodingDim)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mmeanS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencodingDim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mlogstdevS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencodingDim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmeanS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogstdevS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmeanS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogstdevS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sample\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/ml/JupyterHub/conda-env-20200827-2252/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/ml/JupyterHub/conda-env-20200827-2252/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m             if all([s is not None\n\u001b[1;32m    505\u001b[0m                     for s in to_list(input_shape)]):\n\u001b[0;32m--> 506\u001b[0;31m                 \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/ml/JupyterHub/conda-env-20200827-2252/lib/python3.6/site-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    672\u001b[0m                     xs = [K.placeholder(shape=shape, dtype=dtype)\n\u001b[1;32m    673\u001b[0m                           for shape, dtype in zip(input_shape, self._input_dtypes)]\n\u001b[0;32m--> 674\u001b[0;31m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/ml/JupyterHub/conda-env-20200827-2252/lib/python3.6/site-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-198a871d77cc>\u001b[0m in \u001b[0;36msampling\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogstdev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogstdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencodingDim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogstdev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/ml/JupyterHub/conda-env-20200827-2252/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mrandom_normal\u001b[0;34m(shape, mean, stddev, dtype, seed)\u001b[0m\n\u001b[1;32m   4327\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4328\u001b[0m         return tf_keras_backend.random_normal(\n\u001b[0;32m-> 4329\u001b[0;31m             shape, mean=mean, stddev=stddev, dtype=dtype, seed=seed)\n\u001b[0m\u001b[1;32m   4330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/ml/JupyterHub/conda-env-20200827-2252/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mrandom_normal\u001b[0;34m(shape, mean, stddev, dtype, seed)\u001b[0m\n\u001b[1;32m   5600\u001b[0m     \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5601\u001b[0m   return random_ops.random_normal(\n\u001b[0;32m-> 5602\u001b[0;31m       shape, mean=mean, stddev=stddev, dtype=dtype, seed=seed)\n\u001b[0m\u001b[1;32m   5603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/ml/JupyterHub/conda-env-20200827-2252/lib/python3.6/site-packages/tensorflow_core/python/ops/random_ops.py\u001b[0m in \u001b[0;36mrandom_normal\u001b[0;34m(shape, mean, stddev, dtype, seed, name)\u001b[0m\n\u001b[1;32m     67\u001b[0m   \"\"\"\n\u001b[1;32m     68\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"random_normal\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mshape_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mmean_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mstddev_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstddev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"stddev\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/ml/JupyterHub/conda-env-20200827-2252/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mshape_tensor\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# not convertible to Tensors becasue of mixed content.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m       \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdimension_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/ml/JupyterHub/conda-env-20200827-2252/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/ml/JupyterHub/conda-env-20200827-2252/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_autopacking_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1366\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cast_nested_seqs_to_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_autopacking_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"packed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/ml/JupyterHub/conda-env-20200827-2252/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_autopacking_helper\u001b[0;34m(list_or_tuple, dtype, name)\u001b[0m\n\u001b[1;32m   1302\u001b[0m           elems_as_tensors.append(\n\u001b[1;32m   1303\u001b[0m               constant_op.constant(elem, dtype=dtype, name=str(i)))\n\u001b[0;32m-> 1304\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melems_as_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_elems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/ml/JupyterHub/conda-env-20200827-2252/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mpack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   5702\u001b[0m   \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5703\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0;32m-> 5704\u001b[0;31m         \"Pack\", values=values, axis=axis, name=name)\n\u001b[0m\u001b[1;32m   5705\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5706\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/ml/JupyterHub/conda-env-20200827-2252/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    740\u001b[0m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[1;32m    741\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m                                  attrs=attr_protos, op_def=op_def)\n\u001b[0m\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0;31m# `outputs` is returned as a separate return value so that the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/ml/JupyterHub/conda-env-20200827-2252/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    593\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[1;32m    594\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m         compute_device)\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/ml/JupyterHub/conda-env-20200827-2252/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3320\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3321\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3322\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3323\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3324\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/ml/JupyterHub/conda-env-20200827-2252/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1784\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1785\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1786\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1787\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/ml/JupyterHub/conda-env-20200827-2252/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1620\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1621\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1622\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Duplicate node name in graph: 'lambda_1/random_normal/shape'"
     ]
    }
   ],
   "source": [
    "encoder, decoder, autoencoder, sample, loss = makeVAE(16)\n",
    "opt = keras.optimizers.Adam(lr=0.001)\n",
    "autoencoder.compile(optimizer=opt, loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we passed our custom loss function when compiling the model. Next, we will fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=40,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a function to compare original and reconstructed images, which we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showImages(ae, data):\n",
    "    decoded_imgs = ae.predict(data)\n",
    "\n",
    "    n = data.shape[0]  # how many cloth we will display\n",
    "    height = 20\n",
    "    plt.figure(figsize=(height, height/n*2))\n",
    "    for i in range(n):\n",
    "        # display original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(data[i])\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display reconstruction\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(decoded_imgs[i])\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "showImages(autoencoder, x_test[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also sample the latent vectors, which should follow a unit gaussian to generate new cloth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def showImagesGen(decoder, sample=sample, n=20):\n",
    "    latentDim = sample.inputs[0].shape.as_list()[-1]\n",
    "    mean = np.array([0.]*latentDim*n).reshape([n,latentDim])\n",
    "    stdev = np.array([1.]*latentDim*n).reshape([n,latentDim])\n",
    "    decoded_imgs = decoder.predict(sample.predict([mean, stdev]))\n",
    "\n",
    "    height = 20\n",
    "    plt.figure(figsize=(height, height/n))\n",
    "    for i in range(n):\n",
    "        # display reconstruction\n",
    "        ax = plt.subplot(1, n, i + 1)\n",
    "        plt.imshow(decoded_imgs[i])\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showImagesGen(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_latent(encoder, decoder):\n",
    "    # display a n*n 2D manifold of digits\n",
    "    n = 30\n",
    "    digit_size = 28\n",
    "    scale = 2.0\n",
    "    figsize = 15\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-scale, scale, n)\n",
    "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = decoder.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[\n",
    "                i * digit_size : (i + 1) * digit_size,\n",
    "                j * digit_size : (j + 1) * digit_size,\n",
    "            ] = digit\n",
    "\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range + 1\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_latent(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLEASE RUN THIS COMMAND IF YOU FINISHED THE NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "temp=os.getpid()\n",
    "!kill -9 $temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
